# 决策树

## ID3

熵：随机变量的不确定性，越不确定的事物，它的熵就越大。$$H(X) = -\sum\limits_{i=1}^{n}p_i logp_i$$

条件熵：$H(X|Y) = -\sum\limits_{i=1}^{n}p(x_i,y_i)logp(x_i|y_i) = \sum\limits_{j=1}^{n}p(y_j)H(X|y_j)$

信息增益Information Gain：I(X,Y) = H(X)-H(X|Y). 这里X指label，Y指某个特征。

ID3模型：

- 只能处理类别数据，即离散数据
- 节点分裂的时候，分裂所有的值

## C4.5

针对ID3的缺点，C4.5的改进：

- 选择特征的时候用信息增益比代替信息增益
- 能够处理连续值特征
- 能够处理缺失值特征
- 剪枝

### 信息增益比

信息增益比Information Gain Ratio：信息增益和特征熵的比值。特征熵又叫Split Information，$IGR=I_R(D,A) = \frac{I(A,D)}{H_A(D)}$

信息增益（IG）是描述某一个特征属性的存在与否，对总体信息熵的变化量影响。通过 IG 的公式可以推测出，当子树固定时，总的信息熵固定，只要条件熵（就是某一特征属性的信息熵）越小，那么 IG 的值就越大。通过条件熵的计算公式，又可以推测出，如果某一个特性属性的取值越多，那么这个条件熵的值就会越小。从而，采用 IG 最大法选择构建决策，在某一程度上可以理解成选择取值多的特征属性。

${SplitInfo= -\sum_i{p(v_i)}log_2{p(v_i)}}$，其中$v_i$为某一特征属性下的第$i​$个分支属性。注意，在求信息增益的时候，是计算样本label的概率，特征熵是计算样本对应特征的概率。

特征属性取值多，特征熵会比较大，所以信息增益比可以修正信息增益的偏差。比如连续值属性，不同的特征取值可能会非常多，这个时候就需要采用信息增益比来选择特征进行分裂。

### 处理连续值

C4.5处理离散值的方式与ID3一样，分支数是所有的不同取值个数，即完全分裂；**对于连续值，分支数是二叉树**。所以对于连续值的属性，在子树选特征的时候还可以被使用；离散特征不会，因为一旦分裂了，子树的该特征都取值相同，$IG=0$.

#### 选分裂的值

先把连续属性转换为离散属性再进行处理。先对连续值进行排序，然后相邻的两个点取中间值作为分裂点的集合，N个值有N-1个分裂点，对每个分裂点<=和>进行分裂。另外，对于连续属性先进行排序（升序），只有在决策属性发生改变（即label发生了变化）的地方才需要切开，这可以显著减少运算量[2]。

> An optimal binary split is done when it is evaluated the gain only between points of different classes. For example, we don’t have to evaluate the gain between 72 and 75 because they are assigned to the same class ‘Yes’ and this point will surely not be chosen for split. In addition to this, we should not repeat the order of the values at each node, because the order for a node’s children can be determined from the order of the values of the parent. [1]

如何从这至多N-1个分裂点选出最优的那个呢？经证明，在决定连续特征的分界点时采用IG这个指标。因为若采用信息增益率，特征熵影响分裂点信息度量准确性，若某分界点恰好将连续属性分成数目相等的两部分时，特征熵最大，对IGR的影响最大。二叉树使用IGR会倾向于等分，所以我们改用IG来计算，每一种分裂方法，计算两个分支的IG之和。反过来考虑，二叉树的时候分支数被限定为2，所以IG无法倾向于选择取值多的方案。

而选择属性的时候才使用增益率这个指标，选择出最佳分类特征。

#### 选属性

C4.5在选择哪个属性进行分裂的时候，则是通过Information Gain Ratio来选择。对于离散的属性，跟ID3没有大区别；对于连续变量的特征，IGR是所有不同的连续值都考虑进去。例如一颗子树是这样的：

| temperature | 2    | 2    | 2    | 3    | 3    | 6    | 7    | 7    | 7    | 7    |
| ----------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| label       | n    | n    | y    | n    | n    | y    | n    | y    | y    | y    |

温度可理解为连续值，尽管这里是integer格式，这个特征的$IGR = \frac{IG}{SplitInfo}$，$H(X)=-(\frac{5}{10}*\log\frac{5}{10}+\frac{5}{10}*\log\frac{5}{10}) $，$H(X|T)=\frac{3}{10}(-\frac{2}{3}\log\frac{2}{3}-\frac{1}{3}\log\frac{1}{3}) + \frac{2}{10}(-1\log1-0\log0) + \frac{1}{10}(-1\log1) + \frac{4}{10}(-\frac{1}{4}\log\frac{1}{4}-\frac{3}{4}\log\frac{3}{4})$，

假设分解特征为<=2和>2通过上节求得是最优的（IG最大），$SplitInfo(T)=-(\frac{3}{10}\log\frac{3}{10} + \frac{7}{10}\log\frac{7}{10} )$。以上就是这颗子树的温度属性的IGR，我们把所有的属性的IGR都求一遍（无论离散、连续），找到IGR最大的那个即为要进行分裂的属性。

**选择连续属性的哪个分界点，用IG来比较；求得每个连续属性的最优分界点之后，再用IGR来比较属性之间哪个最优。**

#### 对连续属性的处理如下[2]

1.      对特征的取值进行升序排序

2.      两个特征取值之间的中点作为可能的分裂点，将数据集分成两部分，计算每个可能的分裂点的信息增益。优化算法就是只计算分类属性发生改变的那些特征取值。

3.      选择修正后信息增益最大的分裂点作为**该特征的最佳分裂点**

4.      计算最佳分裂点的信息增益率（Gain Ratio）作为特征的Gain Ratio。注意，此处需对最佳分裂点的信息增益进行修正：减去log2(N-1)/|D|（N是连续特征的取值个数，D是训练数据数目，此修正的原因：当离散属性和连续属性并存时，C4.5算法倾向于选择连续特征做最佳树分裂点）



### 处理缺失值

#### 选择属性时

假设10个样本，属性是a,b,c。在计算a属性熵时发现，第10个样本的a属性缺失，那么就把第10个样本去掉，前9个样本组成新的样本集，在新样本集上按正常方法计算a属性的IG，然后结果乘0.9（不含缺失值样本占原始样本的比例），就是a属性最终的IG，即$Gain = ratio*(H(X) - H(X|Y))$ 。求IGR时，$SplitInfo(a) = -\sum_{i=1}^k \frac{|S_i|}{|S|}log_2\frac{|S_i|}{|S|} - \frac{|S_{unknow}|}{|S|}log_2\frac{|S_{unknow}|}{|S|}$，其中$S_{unknow}$是含缺失值的数据组成的样本集。[3]

#### 以概率分裂缺失样本

确定了当前子树的分裂方式（属性+分界点），才会知道子节点样本的数量。把缺失值按照子节点样本出现的概率，分配到各个子节点。

还是上面那个温度的例子，这个子树有10个**无缺失值**的样本，假设选定了温度属性，并按照温度属性<=2和>2为特征，划分了两个子树；假设另外有3个温度有缺失的样本，每个样本按照3/10的权重安排到<=2的子树，7/10的权重安排到>2的子树，其他的10个样本的权重为1。这样后续在算熵的时候，部分样本就带着权重算罢了。

### C4.5模型的特点：

- 多叉树
- 只能用于分类
- 熵模型有对数运算，耗时；处理连续值时要排序，耗时





参考资料：

1. https://octaviansima.wordpress.com/2011/03/25/decision-trees-c4-5/
2. [C4.5 算法对于连续性属性的处理方法介绍](https://blog.csdn.net/shenxiaoming77/article/details/51602976)
3. https://www.quora.com/In-simple-language-how-does-C4-5-deal-with-missing-values


## CART





