
# 决策树

## ID3

熵：随机变量的不确定性，越不确定的事物，它的熵就越大。$$H(X) = -\sum\limits_{i=1}^{n}p_i logp_i$$

条件熵：$H(X|Y) = -\sum\limits_{i=1}^{n}p(x_i,y_i)logp(x_i|y_i) = \sum\limits_{j=1}^{n}p(y_j)H(X|y_j)$

信息增益Information Gain：I(X,Y) = H(X)-H(X|Y). 这里X指label，Y指某个特征。

ID3模型：

- 只能处理类别数据，即离散数据
- 节点分裂的时候，分裂所有的值



## C4.5

针对ID3的缺点，C4.5的改进：

- 选择特征的时候用信息增益比代替信息增益
- 能够处理连续值特征
- 能够处理缺失值特征
- 剪枝

### 信息增益比

信息增益比Information Gain Ratio：信息增益和特征熵的比值。特征熵又叫Split Information，$IGR=I_R(D,A) = \frac{I(A,D)}{H_A(D)}$

信息增益（IG）是描述某一个特征属性的存在与否，对总体信息熵的变化量影响。通过 IG 的公式可以推测出，当子树固定时，总的信息熵固定，只要条件熵（就是某一特征属性的信息熵）越小，那么 IG 的值就越大。通过条件熵的计算公式，又可以推测出，如果某一个特性属性的取值越多，那么这个条件熵的值就会越小。从而，采用 IG 最大法选择构建决策，可以理解成选择取值较多的属性。

${SplitInfo= -\sum_i{p(v_i)}log_2{p(v_i)}}$，其中$v_i$为某一特征属性下的第$i​$个分支属性。注意，在求信息增益的时候，是计算样本label的概率，特征熵是计算样本对应特征的概率。

特征属性取值多，特征熵会比较大，所以信息增益比可以修正信息增益的偏差。比如连续值属性，不同的特征取值可能会非常多，这个时候就需要采用信息增益比来选择特征进行分裂。

### 处理连续值

C4.5处理离散值的方式与ID3一样，分支数是所有的不同取值个数，即完全分裂；**对于连续值，分支数是二叉树**。所以对于连续值的属性，在子树选特征的时候还可以被使用；离散特征不会，因为一旦分裂了，子树的该特征都取值相同，$IG=0$.

#### 选分裂的值

先把连续属性转换为离散属性再进行处理。先对连续值进行排序，然后相邻的两个点取中间值作为分裂点的集合，N个值有N-1个分裂点，对每个分裂点<=和>进行分裂。另外，对于连续属性先进行排序（升序），只有在决策属性发生改变（即label发生了变化）的地方才需要切开，这可以显著减少运算量[2]。

> An optimal binary split is done when it is evaluated the gain only between points of different classes. For example, we don’t have to evaluate the gain between 72 and 75 because they are assigned to the same class ‘Yes’ and this point will surely not be chosen for split. In addition to this, we should not repeat the order of the values at each node, because the order for a node’s children can be determined from the order of the values of the parent. [1]

如何从这至多N-1个分裂点选出最优的那个呢？经证明，在决定连续特征的分界点时采用IG这个指标。因为若采用信息增益率，特征熵影响分裂点信息度量准确性，若某分界点恰好将连续属性分成数目相等的两部分时，特征熵最大，对IGR的影响最大。二叉树使用IGR会倾向于等分，所以我们改用IG来计算，每一种分裂方法，计算两个分支的IG之和。反过来考虑，二叉树的时候分支数被限定为2，所以IG无法倾向于选择取值多的方案。

而选择属性的时候才使用增益率这个指标，选择出最佳分类特征。

#### 选属性

C4.5在选择哪个属性进行分裂的时候，则是通过Information Gain Ratio来选择。对于离散的属性，跟ID3没有大区别；对于连续变量的特征，IGR是所有不同的连续值都考虑进去。例如一颗子树是这样的：

| temperature | 2    | 2    | 2    | 3    | 3    | 6    | 7    | 7    | 7    | 7    |
| ----------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| label       | n    | n    | y    | n    | n    | y    | n    | y    | y    | y    |

温度可理解为连续值，尽管这里是integer格式，这个特征的$IGR = \frac{IG}{SplitInfo}$，$H(X)=-(\frac{5}{10}*\log\frac{5}{10}+\frac{5}{10}*\log\frac{5}{10}) $，$H(X|T)=\frac{3}{10}(-\frac{2}{3}\log\frac{2}{3}-\frac{1}{3}\log\frac{1}{3}) + \frac{2}{10}(-1\log1-0\log0) + \frac{1}{10}(-1\log1) + \frac{4}{10}(-\frac{1}{4}\log\frac{1}{4}-\frac{3}{4}\log\frac{3}{4})$，

假设分解特征为<=2和>2通过上节求得是最优的（IG最大），$SplitInfo(T)=-(\frac{3}{10}\log\frac{3}{10} + \frac{7}{10}\log\frac{7}{10} )$。以上就是这颗子树的温度属性的IGR，我们把所有的属性的IGR都求一遍（无论离散、连续），找到IGR最大的那个即为要进行分裂的属性。

**选择连续属性的哪个分界点，用IG来比较；求得每个连续属性的最优分界点之后，再用IGR来比较属性之间哪个最优。**

#### 对连续属性的处理如下[2]

1. 对特征的取值进行升序排序
2. 两个特征取值之间的中点作为可能的分裂点，将数据集分成两部分，计算每个可能的分裂点的信息增益。优化算法就是只计算分类属性发生改变的那些特征取值
3. 选择修正后信息增益最大的分裂点作为**该特征的最佳分裂点**
4. 计算最佳分裂点的信息增益率（Gain Ratio）作为特征的Gain Ratio。注意，此处需对最佳分裂点的信息增益进行修正：减去log2(N-1)/|D|（N是连续特征的取值个数，D是训练数据数目，此修正的原因：当离散属性和连续属性并存时，C4.5算法倾向于选择连续特征做最佳树分裂点）

### 处理缺失值

#### 选择属性时

假设10个样本，属性是a,b,c。在计算a属性熵时发现，第10个样本的a属性缺失，那么就把第10个样本去掉，前9个样本组成新的样本集，在新样本集上按正常方法计算a属性的IG，然后结果乘0.9（不含缺失值样本占原始样本的比例），就是a属性最终的IG，即$Gain = ratio*(H(X) - H(X|Y))$ 。求IGR时，$SplitInfo(a) = -\sum_{i=1}^k \frac{|S_i|}{|S|}log_2\frac{|S_i|}{|S|} - \frac{|S_{unknow}|}{|S|}log_2\frac{|S_{unknow}|}{|S|}$，其中$S_{unknow}$是含缺失值的数据组成的样本集。[3]

#### 以概率分裂缺失样本

确定了当前子树的分裂方式（属性+分界点），才会知道子节点样本的数量。把缺失值按照子节点样本出现的概率，分配到各个子节点。

还是上面那个温度的例子，这个子树有10个**无缺失值**的样本，假设选定了温度属性，并按照温度属性<=2和>2为特征，划分了两个子树；假设另外有3个温度有缺失的样本，每个样本按照3/10的权重安排到<=2的子树，7/10的权重安排到>2的子树，其他的10个样本的权重为1。这样后续在算熵的时候，部分样本就带着权重算罢了。

#### 剪枝

[4]介绍了一点简单的简直方法；另外C4.5主要采用后剪枝中的悲观剪枝法。

### C4.5模型的特点：

- 多叉树
- 只能用于分类
- 熵模型有对数运算，耗时；处理连续值时要排序，耗时



## CART

针对C4.5的缺点，CART的改进：[5]

- 简化为二叉树
- 可以处理分类，也可以处理回归
- 使用Gini系数来代替信息增益率，不再使用复杂度高的熵模型

### Gini系数

在**分类问题**中，假设有K个类别，第k个**类别**的概率为$p_k$, 则基尼系数的表达式为$Gini(p) = \sum\limits_{k=1}^{K}p_k(1-p_k) = 1- \sum\limits_{k=1}^{K}p_k^2$. 如果是二类分类问题，如果属于第一个样本**输出**的概率是p，则基尼系数的表达式为$Gini(p) = 2p(1-p)$. 给定样本D，Gini系数为$Gini(D) = 1-\sum\limits_{k=1}^{K}(\frac{|C_k|}{|D|})^2$，其中$C_k$是D中属于第k类的样本集合；$Gini(D,A) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)$，其中A表示按照某一个特征进行二元分裂，得到子集D1和D2。Gini(D)表示样本D的不确定性，Gini(D,A)表示按照特征A分裂后样本的不确定性。

比较下基尼系数表达式和熵模型的表达式，二次运算比对数简单很多，基尼系数可以做为熵模型的一个近似替代。Gini系数越大，样本不确定性越大。CART分类树算法就是使用的基尼系数来选择决策树的特征。

为了进一步简化，CART分类树算法每次仅仅对某个特征的值进行二分，而不是多分，这样CART分类树算法建立起来的是二叉树，而不是多叉树。这样一可以进一步简化基尼系数的计算，$Gini(p) = 2p(1-p)​$；二可以建立一个更加优雅的二叉树模型。

### CART分类树

- CART分类树连续值的处理，思想和C4.5相同，都是将连续的特征离散化。唯一的区别在于在选择划分点时的度量方式不同，C4.5使用的是IG，则CART分类树使用的是基尼系数。对于某个连续值属性，同样也是先排序然后离散化，对每个划分点t做二元分类（<=t，>t），但求的是Gini系数，不是IG，**选择Gini系数最小的点**。如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。

- CART分类树离散值的处理，采用的思路是不停的二分离散特征。因为每个节点都是二叉树，所以离散特征一次不会完全分裂，因此后面还有机会在子节点继续选择这个特征。但在ID3和C4.5中，离散属性都是完全分裂，只会参与一次节点的建立。

**CART分类树的步骤：**从根节点开始，用训练集递归的建立CART树。

1. 对于当前节点的数据集为D，如果样本个数小于阈值、或者没有特征、或者样本集D的基尼系数$Gini(D)​$小于阈值，则返回决策树子树，当前节点停止递归。
2. 计算当前节点现有的**各个特征**的**各个特征值**对数据集D的基尼系数$Gini(D,A)​$。缺失值的处理方法和C4.5算法相同。
3. 在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2.
4. 对左右的子节点递归的调用1-4步，生成决策树。
5. 采用叶子节点里概率最大的类别作为当前节点的预测类别。

**注意**，算法计算Gini系数的时候，用到的是两种Gini系数：

- $Gini(D,A)$用于选择特征和分裂点；
- $Gini(D)$用于计算$Gini(D,A)$子集的值，以及判断当前节点是否满足停止递归的条件

具体例子参考李书[6]

### CART回归树

CART回归树和CART分类树大部分类似，所以这里只讨论CART回归树和CART分类树不同之处：

- 连续值的处理方法不同。不使用Gini系数，而是用和方差来度量。对于任意属性A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点：$\underbrace{min}_{A,s}\Bigg[\underbrace{min}_{c_1}\sum\limits_{x_i \in D_1(A,s)}(y_i - c_1)^2 + \underbrace{min}_{c_2}\sum\limits_{x_i \in D_2(A,s)}(y_i - c_2)^2\Bigg]$，其中x为样本，y为样本的输出值，c1为D1数据集的样本输出均值，c2为D2数据集的样本输出均值。
- 决策树建立后做预测的方式不同。回归树输出不是类别，它采用的是用最终叶子的均值或者中位数来预测输出结果。

### CART的剪枝

决策树算法容易对训练集过拟合，CART树进行剪枝，增加决策树的泛化能力。CART采用后剪枝法:

1. 从生成好的决策树（记为$T_0​$）的底端开始，不断剪枝，直到$T_0​$的根节点为止，得到一个子树序列$\{T_0, T_1, \dots,T_n\}​$；
2. 交叉验证，在独立的验证集合上对子树序列测试，从中选择最优子树。

#### 剪枝-得到子树序列

定义损失函数为$C_\alpha(T) = C(T) + \alpha|T|$，其中T为任意子树，C(T)为训练集的预测误差（如Gini系数），|T|为子树叶子节点的个数，$\alpha$为正则化参数，$C_a(T)$是参数为$\alpha$时子树T的整体损失。

对固定的$\alpha$，一定存在使得损失函数$C_{\alpha}(T)$最小的子树（记为$T_{\alpha}$），这个最优子树是唯一的。

> 经证明，用递归的方法对树进行剪枝，将$\alpha$从小增大，$0=\alpha_0<\alpha_1<\dots<\alpha_n, i=0,1,\dots,n$，区间$[\alpha_i,\alpha_{i+1})$对应着序列中的子树$T_i$，$i=0,1,\dots,n$. 序列中的子树是嵌套的。

当$\alpha$很小，甚至等于0的时候，$T_0$即是最优；当$\alpha$增大，最优子树的规模变小；当$\alpha$很大时，单个根节点$T_n$是最优的。

**现在只知道$\alpha_0$和$T_0$，怎么找到序列$\alpha_1,\dots, \alpha_n$和$T_1,\dots, T_n​$呢？**

从完整的树$T_0$开始，对$T_0$的任意内部结点t，作为剪枝的候选：

如果以t为单节点树，$C_{\alpha}(t) = C(t) + \alpha$；以t为根节点的子树，$C_{\alpha}(T_t) = C(T_t) + \alpha|T_t|$.

参数$\alpha$的临界点为$\alpha = \frac{C(t)-C(T_t)}{|T_t|-1}$，这时子树$T_t$跟单节点t的损失函数一样，而t节点更少，所以t比$T_t$更可取，从$T_0$上把$T_t$剪掉只保留t. 

对每个$T_0$上的内部节点都计算$g(t) = \frac{C(t)-C(T_t)}{|T_t|-1}$，在$T_0$上剪去$g(t)$最小的$T_t$，得到$T_1$，并另$\alpha_1 = g(t)$。$T_1$为区间$[\alpha_1,\alpha_2)$的最优子树！

重复此操作，直到得到根节点。

#### 交叉验证得到最优子树

在验证数据集上，比较子树序列$\{T_0, T_1, \dots,T_n\}$，选出Gini系数或平方误差最小的子树。







参考资料：

1. https://octaviansima.wordpress.com/2011/03/25/decision-trees-c4-5/
2. [C4.5 算法对于连续性属性的处理方法介绍](https://blog.csdn.net/shenxiaoming77/article/details/51602976)
3. https://www.quora.com/In-simple-language-how-does-C4-5-deal-with-missing-values
4. [机器学习笔记(6)C4.5决策树中的剪枝处理和Python实现](https://blog.csdn.net/leaf_zizi/article/details/83380081)
5. [决策树算法原理(下)](http://www.cnblogs.com/pinard/p/6053344.html)
6. 李书



决策树overfitting比较的严重，一般要做boosting

