写在最前：这里把通道、channel、feature map泛指同一个意思

## 为什么在Inception模型，1x1卷积核在池化层的后面？

![1x1](../images/inception_1x1.jpg)

左图是最原始的Inception结构，右图是Inception v1的部分结构。为什么对于3x3和5x5卷积层，有一个1x1的卷积层在**前面**，而在池化层的**后面**？

例子1 : GoogleNet中的3a模块
- 输入的feature map是28×28×192
- 1×1卷积通道为64
- 3×3卷积通道为128
- 5×5卷积通道为32

左图卷积核参数：192 × (1×1×64) +192 × (3×3×128) + 192 × (5×5×32) = 387072

右图对3×3和5×5卷积层前分别加入了通道数为96和16的1×1卷积层，这样卷积核参数就变成了:

192 × (1×1×64) +（192×1×1×96+ 96 × 3×3×128）+（192×1×1×16+16×5×5×32）= 157184

**同时在并行pooling层后面加入1×1卷积层后也可以降低输出的feature map数量（feature map尺寸指W、H是共享权值的sliding window，feature map 的数量就是channels）**

左图feature map数量：64 + 128 + 32 + 192(pooling后feature map不变) = 416 （如果每个模块都这样，网络的输出会越来越大）

右图feature map数量：64 + 128 + 32 + 32(pooling后面加了通道为32的1×1卷积) = 256

GoogLeNet利用1×1的卷积降维后，得到了更为紧凑的网络结构，虽然总共有22层，但是参数数量却只是8层的AlexNet的十二分之一（当然也有很大一部分原因是去掉了全连接层）

## 1x1卷积核增加非线性
1*1卷积核，可以在保持feature map尺度不变的（即不损失分辨率）的前提下大幅增加非线性特性（利用后接的非线性激活函数），把网络做的很deep。

备注：一个filter对应卷积后得到一个feature map，不同的filter(不同的weight和bias)，卷积以后得到不同的feature map，提取不同的特征。

## 跨通道信息交互（channal 的变换）

例子：使用1x1卷积核，实现降维和升维的操作其实就是channel间信息的线性组合变化，3x3，64channels的卷积核后面添加一个1x1，28channels的卷积核(ResNet)，就变成了3x3，28channels的卷积核，原来的64个channels就可以理解为跨通道线性组合变成了28channels，这就是通道间的信息交互[7]。

注意：只是在channel维度上做线性组合，W和H上是共享权值的sliding window

## 结论：

1x1卷积核放在卷积层的前面，可以降维，即减少参数。加入1×1卷积后可以降低输入的通道数，卷积核参数、运算复杂度也就跟着降下来了。例如例子中的3×3卷积核，本来是3×3×192×128，输入192，输出128，参数太多；中间加入一个1×1卷积核，通道数为96，变成1×1×192×96+3×3×96×128，参数少很多。

1x1卷积核放在池化层的后面，是为了减少feature map的数量，也是降维。池化层不改变通道数，通道输入多少就输出多少，加一个1×1的卷积层减少一下通道数。

参考链接：
- [1×1 卷积核的作用？（附实例）](https://zhuanlan.zhihu.com/p/35814486)
- [一文读懂卷积神经网络中的1x1卷积核](https://zhuanlan.zhihu.com/p/40050371)
